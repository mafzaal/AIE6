{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae61d7a",
   "metadata": {},
   "source": [
    "# LangGraph Loop Control Mechanisms for Agent Systems\n",
    "\n",
    "This notebook explores the different ways to control loop behavior in agent systems built with LangGraph. We'll cover:\n",
    "\n",
    "1. Setting recursion limits to prevent infinite loops\n",
    "2. Creating interrupt functions to pause execution\n",
    "3. Implementing human-in-the-loop systems\n",
    "4. Building advanced branching logic\n",
    "5. Creating multi-agent coordination systems with controlled loops\n",
    "\n",
    "Let's begin by installing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "#!pip install -q langchain langgraph langchain-openai langsmith tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47e2f7",
   "metadata": {},
   "source": [
    "## Setting Up Environment Variables\n",
    "\n",
    "Before we begin, we need to set up our environment variables for OpenAI API and LangSmith tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25626c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "\n",
    "# Set API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")\n",
    "\n",
    "# Set LangSmith environment variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangGraph Loop Control - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd01f11",
   "metadata": {},
   "source": [
    "## Setting Graph Recursion Limits\n",
    "\n",
    "One of the challenges with agent systems is the risk of infinite loops, where the agent continuously cycles through the same steps without reaching a conclusion. LangGraph provides ways to set recursion limits to prevent this problem.\n",
    "\n",
    "Let's start by creating a basic agent with tools and then add recursion limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3697c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Dict, List, Tuple\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, FunctionMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Define our state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    # We'll add a counter to track the number of iterations\n",
    "    iteration_count: int\n",
    "\n",
    "# Create our tools\n",
    "tavily_tool = TavilySearchResults(max_results=3)\n",
    "tools = [tavily_tool]\n",
    "\n",
    "# Create model with tools\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef3944",
   "metadata": {},
   "source": [
    "Now, we'll define our graph nodes. We'll create:\n",
    "1. An agent node that can call tools\n",
    "2. A tool execution node\n",
    "3. A special node that checks if we've exceeded our iteration limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ef86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state: AgentState) -> Dict:\n",
    "    \"\"\"Process current messages and decide next action.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Increment the iteration count\n",
    "    new_count = state[\"iteration_count\"] + 1\n",
    "    \n",
    "    return {\"messages\": [response], \"iteration_count\": new_count}\n",
    "\n",
    "def tool_node(state: AgentState) -> Dict:\n",
    "    \"\"\"Execute the called tool.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if not last_message.tool_calls:\n",
    "        return {\"messages\": []}\n",
    "    \n",
    "    tool_outputs = []\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        action = tool_call.name\n",
    "        action_input = tool_call.args\n",
    "        \n",
    "        if action == \"tavily_search_results_json\":\n",
    "            output = tavily_tool.invoke(action_input)\n",
    "            tool_outputs.append(\n",
    "                FunctionMessage(\n",
    "                    name=action,\n",
    "                    content=str(output),\n",
    "                    tool_call_id=tool_call.id\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return {\"messages\": tool_outputs}\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Determine if we should continue based on tool calls and iteration count.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    iteration_count = state[\"iteration_count\"]\n",
    "    \n",
    "    # Check if we've reached our maximum number of iterations\n",
    "    if iteration_count >= 5:\n",
    "        print(f\"Reached maximum iterations: {iteration_count}\")\n",
    "        return END\n",
    "    \n",
    "    # Check if we have tool calls or need to end\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool\"\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde2ab3",
   "metadata": {},
   "source": [
    "Now we'll create our graph with the iteration limit built in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0149c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our state graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add our nodes\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"tool\", tool_node)\n",
    "\n",
    "# Add conditional edges\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool\": \"tool\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add a direct edge from tool back to agent\n",
    "graph.add_edge(\"tool\", \"agent\")\n",
    "\n",
    "# Set entry point\n",
    "graph.set_entry_point(\"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "agent_executor = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d20c9",
   "metadata": {},
   "source": [
    "Let's try our agent with iteration limits. We'll create a function to initialize the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09adee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_state(query: str) -> Dict:\n",
    "    \"\"\"Create the initial state for our graph.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"iteration_count\": 0\n",
    "    }\n",
    "\n",
    "# Let's run a query that might require multiple iterations\n",
    "query = \"I need to research current machine learning frameworks for computer vision. Compare at least 3 options and provide details about their capabilities.\"\n",
    "\n",
    "state = create_initial_state(query)\n",
    "result = agent_executor.invoke(state)\n",
    "\n",
    "# Print the final response\n",
    "print(\"Final Response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da604bf8",
   "metadata": {},
   "source": [
    "## Working with Interrupt Functions\n",
    "\n",
    "Sometimes we want our agent to pause execution and wait for additional input before continuing. LangGraph allows us to implement interrupt functions that can pause the execution flow.\n",
    "\n",
    "Let's implement a custom interrupt function that triggers when the agent needs clarification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.base import Checkpoint\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a new state that includes a \"needs_clarification\" flag\n",
    "class AgentStateWithInterrupt(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    iteration_count: int\n",
    "    needs_clarification: bool\n",
    "    clarification_question: str\n",
    "\n",
    "# Define a node that checks if clarification is needed\n",
    "def check_for_clarification(state: AgentStateWithInterrupt) -> Dict:\n",
    "    \"\"\"Check if the agent needs clarification based on its last message.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Create a prompt to check if clarification is needed\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Given the last message from an AI assistant, determine if the AI needs clarification \n",
    "        from the human to continue effectively. If so, formulate a specific question that should be asked.\n",
    "        \n",
    "        Last message: {last_message}\n",
    "        \n",
    "        Output as JSON:\n",
    "        {\n",
    "            \"needs_clarification\": true/false,\n",
    "            \"question\": \"The specific question to ask if clarification is needed\"\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Use the model to determine if clarification is needed\n",
    "    clarification_checker = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    result = clarification_checker.invoke(\n",
    "        prompt.format(last_message=last_message.content)\n",
    "    )\n",
    "    \n",
    "    import json\n",
    "    try:\n",
    "        result_json = json.loads(result.content)\n",
    "        return {\n",
    "            \"needs_clarification\": result_json.get(\"needs_clarification\", False),\n",
    "            \"clarification_question\": result_json.get(\"question\", \"\")\n",
    "        }\n",
    "    except:\n",
    "        return {\"needs_clarification\": False, \"clarification_question\": \"\"}\n",
    "\n",
    "# Define an interrupt function\n",
    "def interrupt_for_clarification(state: AgentStateWithInterrupt, config, runtime, events, **kwargs):\n",
    "    \"\"\"Interrupt execution if clarification is needed.\"\"\"\n",
    "    if state.get(\"needs_clarification\", False):\n",
    "        # This will pause execution and return current state\n",
    "        print(f\"Interrupting for clarification: {state['clarification_question']}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Define a function to handle human input when interrupted\n",
    "def get_human_input(checkpoint: Checkpoint):\n",
    "    \"\"\"Get human input when the graph is interrupted.\"\"\"\n",
    "    state = checkpoint.state\n",
    "    question = state.get(\"clarification_question\", \"Do you have any clarification?\")\n",
    "    \n",
    "    print(f\"Agent is asking: {question}\")\n",
    "    human_input = input(\"Your response: \")\n",
    "    \n",
    "    # Update the state with the human's response\n",
    "    new_messages = state[\"messages\"].copy()\n",
    "    new_messages.append(HumanMessage(content=human_input))\n",
    "    \n",
    "    updated_state = {\n",
    "        **state,\n",
    "        \"messages\": new_messages,\n",
    "        \"needs_clarification\": False,\n",
    "        \"clarification_question\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Resume execution with the updated state\n",
    "    return checkpoint.with_state(updated_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d4474",
   "metadata": {},
   "source": [
    "Now let's create our graph with the interrupt capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d7d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph with interrupt capability\n",
    "interrupt_graph = StateGraph(AgentStateWithInterrupt)\n",
    "\n",
    "# Add nodes\n",
    "interrupt_graph.add_node(\"agent\", agent_node)\n",
    "interrupt_graph.add_node(\"tool\", tool_node)\n",
    "interrupt_graph.add_node(\"check_clarification\", check_for_clarification)\n",
    "\n",
    "# Add conditional edges\n",
    "interrupt_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool\": \"tool\",\n",
    "        END: \"check_clarification\"\n",
    "    }\n",
    ")\n",
    "\n",
    "interrupt_graph.add_edge(\"tool\", \"agent\")\n",
    "interrupt_graph.add_edge(\"check_clarification\", END)\n",
    "\n",
    "# Set entry point\n",
    "interrupt_graph.set_entry_point(\"agent\")\n",
    "\n",
    "# Add the interrupt\n",
    "interrupt_graph.add_interrupt(\n",
    "    interrupt_for_clarification, \n",
    "    {\n",
    "        # This specifies when the interrupt can trigger (after any node)\n",
    "        \"agent\": True,\n",
    "        \"tool\": True,\n",
    "        \"check_clarification\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "interruptible_agent = interrupt_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a17a8e",
   "metadata": {},
   "source": [
    "## Implementing Human-in-the-Loop Systems\n",
    "\n",
    "Now let's put everything together to build a human-in-the-loop system that can handle interrupts for clarification while also maintaining recursion limits.\n",
    "\n",
    "We'll implement a more complete system that can:\n",
    "1. Process user queries\n",
    "2. Use tools when needed\n",
    "3. Ask for clarification when uncertain\n",
    "4. Respect iteration limits to prevent infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an enhanced state for our human-in-the-loop system\n",
    "class EnhancedAgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    iteration_count: int\n",
    "    needs_human_input: bool\n",
    "    human_input_prompt: str\n",
    "    max_iterations: int\n",
    "\n",
    "# Enhanced agent node that can recognize when it needs human input\n",
    "def enhanced_agent_node(state: EnhancedAgentState) -> Dict:\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Check if we have a special system message to request clarification\n",
    "    last_user_msg_idx = -1\n",
    "    for i, msg in enumerate(reversed(messages)):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_user_msg_idx = len(messages) - i - 1\n",
    "            break\n",
    "    \n",
    "    # Create a system message to encourage asking for clarification when needed\n",
    "    system_msg = (\"You are a helpful assistant that can use tools to answer questions. \"\n",
    "                 \"If you're uncertain about the query or need more information to provide \"\n",
    "                 \"a complete answer, explicitly state that you need clarification and \"\n",
    "                 \"what specific information would help. Use the format 'I need clarification: <question>'\")\n",
    "    \n",
    "    augmented_messages = messages.copy()\n",
    "    if len(messages) > 0 and isinstance(messages[0], HumanMessage):\n",
    "        augmented_messages.insert(0, AIMessage(content=system_msg))\n",
    "    \n",
    "    response = model.invoke(augmented_messages)\n",
    "    \n",
    "    # Increment the iteration count\n",
    "    new_count = state[\"iteration_count\"] + 1\n",
    "    \n",
    "    # Check if the response indicates a need for clarification\n",
    "    needs_input = False\n",
    "    input_prompt = \"\"\n",
    "    \n",
    "    if \"I need clarification:\" in response.content:\n",
    "        needs_input = True\n",
    "        # Extract the clarification question\n",
    "        parts = response.content.split(\"I need clarification:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            input_prompt = parts[1].strip()\n",
    "        else:\n",
    "            input_prompt = \"Can you provide more information?\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response], \n",
    "        \"iteration_count\": new_count,\n",
    "        \"needs_human_input\": needs_input,\n",
    "        \"human_input_prompt\": input_prompt\n",
    "    }\n",
    "\n",
    "# Enhanced tool node that maintains our state fields\n",
    "def enhanced_tool_node(state: EnhancedAgentState) -> Dict:\n",
    "    tool_results = tool_node(state)\n",
    "    \n",
    "    # Maintain the other state values\n",
    "    return {\n",
    "        **tool_results,\n",
    "        \"iteration_count\": state[\"iteration_count\"],\n",
    "        \"needs_human_input\": state[\"needs_human_input\"],\n",
    "        \"human_input_prompt\": state[\"human_input_prompt\"],\n",
    "        \"max_iterations\": state[\"max_iterations\"]\n",
    "    }\n",
    "\n",
    "# Enhanced decision function that checks both iteration limit and need for human input\n",
    "def enhanced_should_continue(state: EnhancedAgentState) -> str:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    iteration_count = state[\"iteration_count\"]\n",
    "    max_iterations = state[\"max_iterations\"]\n",
    "    \n",
    "    # Check if we need human input\n",
    "    if state[\"needs_human_input\"]:\n",
    "        return \"need_human_input\"\n",
    "    \n",
    "    # Check if we've reached our maximum iterations\n",
    "    if iteration_count >= max_iterations:\n",
    "        print(f\"Reached maximum iterations: {iteration_count}/{max_iterations}\")\n",
    "        return END\n",
    "    \n",
    "    # Check if we have tool calls or need to end\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# Human input node\n",
    "def human_input_node(state: EnhancedAgentState) -> Dict:\n",
    "    prompt = state[\"human_input_prompt\"]\n",
    "    print(f\"\\nAgent needs clarification: {prompt}\")\n",
    "    \n",
    "    user_input = input(\"Your response: \")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=user_input)],\n",
    "        \"needs_human_input\": False,\n",
    "        \"human_input_prompt\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3187de",
   "metadata": {},
   "source": [
    "Now let's create our human-in-the-loop graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37150eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the human-in-the-loop graph\n",
    "hitl_graph = StateGraph(EnhancedAgentState)\n",
    "\n",
    "# Add nodes\n",
    "hitl_graph.add_node(\"agent\", enhanced_agent_node)\n",
    "hitl_graph.add_node(\"tool\", enhanced_tool_node)\n",
    "hitl_graph.add_node(\"human_input\", human_input_node)\n",
    "\n",
    "# Add conditional edges\n",
    "hitl_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    enhanced_should_continue,\n",
    "    {\n",
    "        \"tool\": \"tool\",\n",
    "        \"need_human_input\": \"human_input\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "hitl_graph.add_edge(\"tool\", \"agent\")\n",
    "hitl_graph.add_edge(\"human_input\", \"agent\")\n",
    "\n",
    "# Set entry point\n",
    "hitl_graph.set_entry_point(\"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "hitl_agent = hitl_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d9fff",
   "metadata": {},
   "source": [
    "Let's test our human-in-the-loop system with a query that might require clarification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hitl_initial_state(query: str, max_iterations: int = 5) -> Dict:\n",
    "    \"\"\"Create the initial state for our human-in-the-loop graph.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"iteration_count\": 0,\n",
    "        \"needs_human_input\": False,\n",
    "        \"human_input_prompt\": \"\",\n",
    "        \"max_iterations\": max_iterations\n",
    "    }\n",
    "\n",
    "# Run a query that might need clarification\n",
    "ambiguous_query = \"I need to implement that algorithm we discussed yesterday. What steps should I follow?\"\n",
    "\n",
    "hitl_state = create_hitl_initial_state(ambiguous_query)\n",
    "hitl_result = hitl_agent.invoke(hitl_state)\n",
    "\n",
    "# Print the final response\n",
    "print(\"\\nFinal Response:\")\n",
    "print(hitl_result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4942574",
   "metadata": {},
   "source": [
    "## Building Advanced Branching Logic\n",
    "\n",
    "LangGraph allows us to create complex branching logic that can handle different conditions and direct the flow of execution accordingly. Let's build a system with advanced branching that combines recursion limits with multiple conditional paths.\n",
    "\n",
    "Our enhanced system will:\n",
    "1. Route different types of queries to different processing paths\n",
    "2. Apply different recursion limits based on the query type\n",
    "3. Handle different error conditions gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f082c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "# Define query types as an enum for better type hinting\n",
    "class QueryType(str, Enum):\n",
    "    FACTUAL = \"factual\"\n",
    "    CREATIVE = \"creative\"\n",
    "    ANALYSIS = \"analysis\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "# Define state for advanced branching\n",
    "class AdvancedBranchingState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    iteration_count: int\n",
    "    query_type: str\n",
    "    max_iterations: int\n",
    "    error_count: int\n",
    "\n",
    "# Node to classify the query type\n",
    "def classify_query_node(state: AdvancedBranchingState) -> Dict:\n",
    "    \"\"\"Classify the query type to determine the appropriate processing path.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if not messages:\n",
    "        return {\"query_type\": QueryType.UNKNOWN}\n",
    "    \n",
    "    last_user_message = None\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_user_message = msg.content\n",
    "            break\n",
    "    \n",
    "    if not last_user_message:\n",
    "        return {\"query_type\": QueryType.UNKNOWN}\n",
    "    \n",
    "    # Use a simple classifier model to determine query type\n",
    "    classifier_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Classify the following user query into one of these categories:\n",
    "        - factual: Seeking factual information or data\n",
    "        - creative: Asking for creative content or ideas\n",
    "        - analysis: Requesting analysis or interpretation of information\n",
    "        \n",
    "        User query: {query}\n",
    "        \n",
    "        Return only one word: factual, creative, or analysis.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    classifier = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    result = classifier.invoke(\n",
    "        classifier_prompt.format(query=last_user_message)\n",
    "    )\n",
    "    \n",
    "    # Set different max iterations based on query type\n",
    "    query_type = result.content.strip().lower()\n",
    "    max_iterations = {\n",
    "        QueryType.FACTUAL: 3,   # Factual queries need fewer iterations\n",
    "        QueryType.CREATIVE: 5,  # Creative tasks get more iterations\n",
    "        QueryType.ANALYSIS: 7,  # Analysis gets the most iterations\n",
    "    }.get(query_type, 4)  # Default value\n",
    "    \n",
    "    return {\n",
    "        \"query_type\": query_type,\n",
    "        \"max_iterations\": max_iterations\n",
    "    }\n",
    "\n",
    "# Specialized node for handling factual queries\n",
    "def factual_processing_node(state: AdvancedBranchingState) -> Dict:\n",
    "    \"\"\"Process factual queries with emphasis on accuracy.\"\"\"\n",
    "    factual_model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind_tools(tools)\n",
    "    \n",
    "    system_message = (\"You are a factual assistant. Always cite your sources and be precise. \"\n",
    "                     \"When uncertain, acknowledge limitations in your knowledge.\")\n",
    "    \n",
    "    messages = state[\"messages\"].copy()\n",
    "    if messages and isinstance(messages[0], HumanMessage):\n",
    "        messages.insert(0, AIMessage(content=system_message))\n",
    "    \n",
    "    response = factual_model.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"iteration_count\": state[\"iteration_count\"] + 1\n",
    "    }\n",
    "\n",
    "# Specialized node for handling creative queries\n",
    "def creative_processing_node(state: AdvancedBranchingState) -> Dict:\n",
    "    \"\"\"Process creative queries with more flexibility.\"\"\"\n",
    "    creative_model = ChatOpenAI(model=\"gpt-4\", temperature=0.7).bind_tools(tools)\n",
    "    \n",
    "    system_message = (\"You are a creative assistant. Think outside the box and provide unique perspectives. \"\n",
    "                     \"Don't hesitate to explore unconventional ideas.\")\n",
    "    \n",
    "    messages = state[\"messages\"].copy()\n",
    "    if messages and isinstance(messages[0], HumanMessage):\n",
    "        messages.insert(0, AIMessage(content=system_message))\n",
    "    \n",
    "    response = creative_model.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"iteration_count\": state[\"iteration_count\"] + 1\n",
    "    }\n",
    "\n",
    "# Specialized node for handling analysis queries\n",
    "def analysis_processing_node(state: AdvancedBranchingState) -> Dict:\n",
    "    \"\"\"Process analysis queries with deeper reasoning.\"\"\"\n",
    "    analysis_model = ChatOpenAI(model=\"gpt-4\", temperature=0.2).bind_tools(tools)\n",
    "    \n",
    "    system_message = (\"You are an analytical assistant. Break down complex problems step by step. \"\n",
    "                     \"Consider different perspectives and evaluate evidence carefully.\")\n",
    "    \n",
    "    messages = state[\"messages\"].copy()\n",
    "    if messages and isinstance(messages[0], HumanMessage):\n",
    "        messages.insert(0, AIMessage(content=system_message))\n",
    "    \n",
    "    response = analysis_model.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"iteration_count\": state[\"iteration_count\"] + 1\n",
    "    }\n",
    "\n",
    "# Route to the appropriate processing node based on query type\n",
    "def route_by_query_type(state: AdvancedBranchingState) -> str:\n",
    "    \"\"\"Route to the appropriate node based on query type.\"\"\"\n",
    "    query_type = state.get(\"query_type\", QueryType.UNKNOWN)\n",
    "    \n",
    "    routing_map = {\n",
    "        QueryType.FACTUAL: \"factual_processing\",\n",
    "        QueryType.CREATIVE: \"creative_processing\",\n",
    "        QueryType.ANALYSIS: \"analysis_processing\"\n",
    "    }\n",
    "    \n",
    "    return routing_map.get(query_type, \"factual_processing\")\n",
    "\n",
    "# Enhanced should continue function\n",
    "def advanced_should_continue(state: AdvancedBranchingState) -> str:\n",
    "    \"\"\"Determine next step based on multiple conditions.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    iteration_count = state[\"iteration_count\"]\n",
    "    max_iterations = state[\"max_iterations\"]\n",
    "    error_count = state.get(\"error_count\", 0)\n",
    "    \n",
    "    # Check for error conditions (too many errors leads to end)\n",
    "    if error_count >= 2:\n",
    "        return \"error_handling\"\n",
    "    \n",
    "    # Check iteration limit\n",
    "    if iteration_count >= max_iterations:\n",
    "        return END\n",
    "    \n",
    "    # Check if we have tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool\"\n",
    "    \n",
    "    return END\n",
    "\n",
    "# Error handling node\n",
    "def error_handling_node(state: AdvancedBranchingState) -> Dict:\n",
    "    \"\"\"Handle error conditions gracefully.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"I apologize, but I'm having difficulty processing this request. \"\n",
    "                               \"Let me try a different approach or please consider rephrasing your question.\")],\n",
    "        \"error_count\": 0  # Reset error count after handling\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d97812",
   "metadata": {},
   "source": [
    "Let's create our advanced branching graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bffbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the advanced branching graph\n",
    "branching_graph = StateGraph(AdvancedBranchingState)\n",
    "\n",
    "# Add nodes\n",
    "branching_graph.add_node(\"classifier\", classify_query_node)\n",
    "branching_graph.add_node(\"factual_processing\", factual_processing_node)\n",
    "branching_graph.add_node(\"creative_processing\", creative_processing_node)\n",
    "branching_graph.add_node(\"analysis_processing\", analysis_processing_node)\n",
    "branching_graph.add_node(\"tool\", enhanced_tool_node)\n",
    "branching_graph.add_node(\"error_handling\", error_handling_node)\n",
    "\n",
    "# Set entry point - start by classifying the query\n",
    "branching_graph.set_entry_point(\"classifier\")\n",
    "\n",
    "# Add edges from classifier to processing nodes\n",
    "branching_graph.add_conditional_edges(\n",
    "    \"classifier\",\n",
    "    route_by_query_type,\n",
    "    {\n",
    "        \"factual_processing\": \"factual_processing\",\n",
    "        \"creative_processing\": \"creative_processing\",\n",
    "        \"analysis_processing\": \"analysis_processing\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add conditional edges from processing nodes\n",
    "for node in [\"factual_processing\", \"creative_processing\", \"analysis_processing\"]:\n",
    "    branching_graph.add_conditional_edges(\n",
    "        node,\n",
    "        advanced_should_continue,\n",
    "        {\n",
    "            \"tool\": \"tool\",\n",
    "            \"error_handling\": \"error_handling\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Tool node goes back to the appropriate processing node based on query type\n",
    "branching_graph.add_conditional_edges(\n",
    "    \"tool\",\n",
    "    route_by_query_type,\n",
    "    {\n",
    "        \"factual_processing\": \"factual_processing\",\n",
    "        \"creative_processing\": \"creative_processing\",\n",
    "        \"analysis_processing\": \"analysis_processing\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Error handling goes back to classifier to retry with a clean slate\n",
    "branching_graph.add_edge(\"error_handling\", END)\n",
    "\n",
    "# Compile the graph\n",
    "advanced_agent = branching_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a44ab",
   "metadata": {},
   "source": [
    "Let's test our advanced branching agent with different types of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_branching_initial_state(query: str) -> Dict:\n",
    "    \"\"\"Create the initial state for advanced branching graph.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"iteration_count\": 0,\n",
    "        \"query_type\": QueryType.UNKNOWN,\n",
    "        \"max_iterations\": 5,\n",
    "        \"error_count\": 0\n",
    "    }\n",
    "\n",
    "# Test with different query types\n",
    "queries = [\n",
    "    \"What is the average temperature on Mars?\",  # Factual\n",
    "    \"Write a short sci-fi story about AI in the year 2100\",  # Creative\n",
    "    \"Analyze the potential economic impacts of quantum computing\",  # Analysis\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"\\n===== Testing Query {i+1} =====\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    state = create_branching_initial_state(query)\n",
    "    result = advanced_agent.invoke(state)\n",
    "    \n",
    "    print(f\"Query classified as: {result['query_type']}\")\n",
    "    print(f\"Iterations used: {result['iteration_count']}/{result['max_iterations']}\")\n",
    "    print(\"\\nFinal Response:\")\n",
    "    print(result[\"messages\"][-1].content)\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf0d4a",
   "metadata": {},
   "source": [
    "## Creating Multi-Agent Coordination Systems\n",
    "\n",
    "Finally, let's build a system where multiple specialized agents work together, coordinated by a central manager, with controlled loops and coordination mechanisms.\n",
    "\n",
    "Our multi-agent system will include:\n",
    "1. A coordinator agent that delegates tasks and synthesizes results\n",
    "2. Specialized agents for different subtasks\n",
    "3. Loop controls to prevent infinite delegation\n",
    "4. State tracking between different agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a469a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from operator import itemgetter\n",
    "\n",
    "# Define state for multi-agent system\n",
    "class MultiAgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    coordinator_state: dict\n",
    "    research_state: dict\n",
    "    creative_state: dict\n",
    "    analyst_state: dict\n",
    "    current_agent: str\n",
    "    delegation_count: int\n",
    "    max_delegations: int\n",
    "\n",
    "# Coordinator agent that delegates and synthesizes\n",
    "def coordinator_node(state: MultiAgentState) -> Dict:\n",
    "    \"\"\"Coordinator agent that delegates tasks and synthesizes results.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    delegation_count = state[\"delegation_count\"]\n",
    "    max_delegations = state[\"max_delegations\"]\n",
    "    \n",
    "    # Create a system message for the coordinator\n",
    "    system_msg = (\n",
    "        \"You are a coordination agent that breaks down complex tasks and delegates them to specialized agents. \"\n",
    "        \"Your team includes:\\n\"\n",
    "        \"1. Research Agent: Good at finding factual information\\n\"\n",
    "        \"2. Creative Agent: Good at generating creative content\\n\"\n",
    "        \"3. Analyst Agent: Good at critical thinking and analysis\\n\\n\"\n",
    "        \"When delegating, use the format: DELEGATE_TO_[AGENT]: [specific task]\\n\"\n",
    "        \"You can delegate to RESEARCH, CREATIVE, or ANALYST.\\n\"\n",
    "        \"When you have a complete answer, do NOT use the DELEGATE format.\"\n",
    "    )\n",
    "    \n",
    "    coordinator_model = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    \n",
    "    coordinator_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_msg),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    \n",
    "    # If we've delegated too many times, force synthesis\n",
    "    forced_synthesis = \"\"\n",
    "    if delegation_count >= max_delegations:\n",
    "        forced_synthesis = (\"\\n\\nYou've delegated the maximum number of times. \"\n",
    "                           \"Please synthesize the information you have and provide a final answer.\")\n",
    "    \n",
    "    response = coordinator_model.invoke(\n",
    "        coordinator_prompt.format(\n",
    "            messages=messages + ([AIMessage(content=forced_synthesis)] if forced_synthesis else [])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update coordinator state\n",
    "    coordinator_state = state.get(\"coordinator_state\", {})\n",
    "    coordinator_state[\"last_response\"] = response.content\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"coordinator_state\": coordinator_state\n",
    "    }\n",
    "\n",
    "# Research agent specialized in factual information\n",
    "def research_agent_node(state: MultiAgentState) -> Dict:\n",
    "    \"\"\"Research agent that handles factual queries.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    research_state = state.get(\"research_state\", {})\n",
    "    \n",
    "    # Get the delegated task\n",
    "    last_message = messages[-1]\n",
    "    task = last_message.content\n",
    "    if \"DELEGATE_TO_RESEARCH:\" in task:\n",
    "        task = task.split(\"DELEGATE_TO_RESEARCH:\", 1)[1].strip()\n",
    "    \n",
    "    system_msg = (\n",
    "        \"You are a research agent specialized in finding accurate factual information. \"\n",
    "        \"Use tools when necessary and cite sources. Be concise but thorough.\"\n",
    "    )\n",
    "    \n",
    "    research_model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind_tools(tools)\n",
    "    \n",
    "    research_messages = [\n",
    "        AIMessage(content=system_msg),\n",
    "        HumanMessage(content=task)\n",
    "    ]\n",
    "    \n",
    "    response = research_model.invoke(research_messages)\n",
    "    \n",
    "    # Update research state\n",
    "    research_state[\"last_task\"] = task\n",
    "    research_state[\"last_response\"] = response.content\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Research Agent: {response.content}\")],\n",
    "        \"research_state\": research_state,\n",
    "        \"current_agent\": \"coordinator\"  # Return to coordinator\n",
    "    }\n",
    "\n",
    "# Creative agent specialized in creative content\n",
    "def creative_agent_node(state: MultiAgentState) -> Dict:\n",
    "    \"\"\"Creative agent that handles creative tasks.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    creative_state = state.get(\"creative_state\", {})\n",
    "    \n",
    "    # Get the delegated task\n",
    "    last_message = messages[-1]\n",
    "    task = last_message.content\n",
    "    if \"DELEGATE_TO_CREATIVE:\" in task:\n",
    "        task = task.split(\"DELEGATE_TO_CREATIVE:\", 1)[1].strip()\n",
    "    \n",
    "    system_msg = (\n",
    "        \"You are a creative agent with a flair for imagination and originality. \"\n",
    "        \"Generate engaging, unique content that captivates the audience.\"\n",
    "    )\n",
    "    \n",
    "    creative_model = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "    \n",
    "    creative_messages = [\n",
    "        AIMessage(content=system_msg),\n",
    "        HumanMessage(content=task)\n",
    "    ]\n",
    "    \n",
    "    response = creative_model.invoke(creative_messages)\n",
    "    \n",
    "    # Update creative state\n",
    "    creative_state[\"last_task\"] = task\n",
    "    creative_state[\"last_response\"] = response.content\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Creative Agent: {response.content}\")],\n",
    "        \"creative_state\": creative_state,\n",
    "        \"current_agent\": \"coordinator\"  # Return to coordinator\n",
    "    }\n",
    "\n",
    "# Analyst agent specialized in critical thinking\n",
    "def analyst_agent_node(state: MultiAgentState) -> Dict:\n",
    "    \"\"\"Analyst agent that handles analysis tasks.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    analyst_state = state.get(\"analyst_state\", {})\n",
    "    \n",
    "    # Get the delegated task\n",
    "    last_message = messages[-1]\n",
    "    task = last_message.content\n",
    "    if \"DELEGATE_TO_ANALYST:\" in task:\n",
    "        task = task.split(\"DELEGATE_TO_ANALYST:\", 1)[1].strip()\n",
    "    \n",
    "    system_msg = (\n",
    "        \"You are an analytical agent with strong critical thinking skills. \"\n",
    "        \"Break down complex problems, evaluate information objectively, and provide insightful analysis.\"\n",
    "    )\n",
    "    \n",
    "    analyst_model = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
    "    \n",
    "    analyst_messages = [\n",
    "        AIMessage(content=system_msg),\n",
    "        HumanMessage(content=task)\n",
    "    ]\n",
    "    \n",
    "    response = analyst_model.invoke(analyst_messages)\n",
    "    \n",
    "    # Update analyst state\n",
    "    analyst_state[\"last_task\"] = task\n",
    "    analyst_state[\"last_response\"] = response.content\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Analyst Agent: {response.content}\")],\n",
    "        \"analyst_state\": analyst_state,\n",
    "        \"current_agent\": \"coordinator\"  # Return to coordinator\n",
    "    }\n",
    "\n",
    "# Router function to determine which agent should handle the task\n",
    "def route_to_agent(state: MultiAgentState) -> str:\n",
    "    \"\"\"Route to the appropriate agent based on delegation instructions.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if not messages:\n",
    "        return \"coordinator\"\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    content = last_message.content\n",
    "    \n",
    "    # Increment delegation count if this is a delegation\n",
    "    new_state = {}\n",
    "    if any(delegation in content for delegation in [\"DELEGATE_TO_RESEARCH:\", \"DELEGATE_TO_CREATIVE:\", \"DELEGATE_TO_ANALYST:\"]):\n",
    "        new_state[\"delegation_count\"] = state[\"delegation_count\"] + 1\n",
    "    \n",
    "    # Check for explicit delegations\n",
    "    if \"DELEGATE_TO_RESEARCH:\" in content:\n",
    "        return \"research_agent\"\n",
    "    elif \"DELEGATE_TO_CREATIVE:\" in content:\n",
    "        return \"creative_agent\"\n",
    "    elif \"DELEGATE_TO_ANALYST:\" in content:\n",
    "        return \"analyst_agent\"\n",
    "    \n",
    "    # If we've reached max delegations or no delegation instruction, we're done\n",
    "    if state[\"delegation_count\"] >= state[\"max_delegations\"]:\n",
    "        return END\n",
    "    \n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed08eb5",
   "metadata": {},
   "source": [
    "Let's create our multi-agent coordination system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03747956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multi-agent graph\n",
    "multi_agent_graph = StateGraph(MultiAgentState)\n",
    "\n",
    "# Add nodes\n",
    "multi_agent_graph.add_node(\"coordinator\", coordinator_node)\n",
    "multi_agent_graph.add_node(\"research_agent\", research_agent_node)\n",
    "multi_agent_graph.add_node(\"creative_agent\", creative_agent_node)\n",
    "multi_agent_graph.add_node(\"analyst_agent\", analyst_agent_node)\n",
    "\n",
    "# Set entry point\n",
    "multi_agent_graph.set_entry_point(\"coordinator\")\n",
    "\n",
    "# Add conditional edges from coordinator to specialized agents\n",
    "multi_agent_graph.add_conditional_edges(\n",
    "    \"coordinator\",\n",
    "    route_to_agent,\n",
    "    {\n",
    "        \"research_agent\": \"research_agent\",\n",
    "        \"creative_agent\": \"creative_agent\",\n",
    "        \"analyst_agent\": \"analyst_agent\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges from specialized agents back to coordinator\n",
    "multi_agent_graph.add_edge(\"research_agent\", \"coordinator\")\n",
    "multi_agent_graph.add_edge(\"creative_agent\", \"coordinator\")\n",
    "multi_agent_graph.add_edge(\"analyst_agent\", \"coordinator\")\n",
    "\n",
    "# Compile the graph\n",
    "multi_agent_system = multi_agent_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36433e",
   "metadata": {},
   "source": [
    "Let's test our multi-agent system with a complex task that might require multiple types of agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_agent_initial_state(query: str, max_delegations: int = 5) -> Dict:\n",
    "    \"\"\"Create the initial state for multi-agent system.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"coordinator_state\": {},\n",
    "        \"research_state\": {},\n",
    "        \"creative_state\": {},\n",
    "        \"analyst_state\": {},\n",
    "        \"current_agent\": \"coordinator\",\n",
    "        \"delegation_count\": 0,\n",
    "        \"max_delegations\": max_delegations\n",
    "    }\n",
    "\n",
    "# Test with a complex query that requires multiple agent types\n",
    "complex_query = \"\"\"\n",
    "I'm planning a presentation on the future of AI. I need:\n",
    "1. Recent facts about AI advancement trends\n",
    "2. A creative metaphor to explain AI to non-technical people\n",
    "3. An analysis of potential economic impacts of AI in the next decade\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing Multi-Agent System with a Complex Query\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Query: {complex_query}\")\n",
    "\n",
    "multi_agent_state = create_multi_agent_initial_state(complex_query, max_delegations=3)\n",
    "\n",
    "# Stream the execution to see it step by step\n",
    "print(\"\\nExecution Trace:\")\n",
    "print(\"-\"*50)\n",
    "async for chunk in multi_agent_system.astream(multi_agent_state, stream_mode=\"updates\"):\n",
    "    for node_name, node_state in chunk.items():\n",
    "        print(f\"\\nAgent: {node_name}\")\n",
    "        if \"messages\" in node_state:\n",
    "            for msg in node_state[\"messages\"]:\n",
    "                print(f\"Message: {msg.content[:100]}...\")\n",
    "        print(\"-\"*30)\n",
    "\n",
    "# Get the final result\n",
    "result = multi_agent_system.invoke(multi_agent_state)\n",
    "\n",
    "print(\"\\nFinal Result:\")\n",
    "print(\"=\"*50)\n",
    "print(result[\"messages\"][-1].content)\n",
    "print(\"\\nDelegation count:\", result[\"delegation_count\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
